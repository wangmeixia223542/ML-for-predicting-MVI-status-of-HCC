{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "from onekey_algo import OnekeyDS as okds\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs('img', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('features', exist_ok=True)\n",
    "# 设置数据目录\n",
    "#label_file = r'你自己数据的路径'\n",
    "feature_file = r'C:\\Users\\Admin\\Desktop\\T and V\\HCC.csv'\n",
    "# 对应的标签文件\n",
    "label_file = r'C:\\Users\\Admin\\Desktop\\T and V\\label.csv'\n",
    "#label_file = os.path.join(okds.ct, 'label_multicenter.csv')\n",
    "# 读取标签数据列名\n",
    "labels = ['label']\n",
    "\n",
    "feature_data = pd.read_csv(feature_file)\n",
    "display(feature_data)\n",
    "label_data = pd.read_csv(label_file)\n",
    "label_data.head()\n",
    "\n",
    "from onekey_algo.custom.utils import print_join_info\n",
    "# 删掉ID这一列。\n",
    "print_join_info(feature_data, label_data)\n",
    "combined_data = pd.merge(feature_data, label_data, on=['ID'], how='inner')\n",
    "ids = combined_data['ID']\n",
    "combined_data = combined_data.drop(['ID'], axis=1)\n",
    "print(combined_data[labels].value_counts())\n",
    "combined_data.columns\n",
    "\n",
    "from onekey_algo.custom.components.comp1 import normalize_df\n",
    "data = normalize_df(combined_data, not_norm=labels, group='group')\n",
    "data = data.dropna(axis=1)\n",
    "data.describe()\n",
    "\n",
    "pearson_corr = data[data['group'] == 'train'][[c for c in data.columns if c not in labels]].corr('pearson')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from onekey_algo.custom.components.comp1 import draw_matrix\n",
    "\n",
    "if combined_data.shape[1] < 100:\n",
    "    plt.figure(figsize=(10.0, 8.0))\n",
    "    # 选择可视化的相关系数\n",
    "    draw_matrix(pearson_corr, annot=True, cmap='YlGnBu', cbar=False)\n",
    "    plt.savefig(f'img/feature_corr.svg', bbox_inches = 'tight')\n",
    "    \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if combined_data.shape[1] < 100:\n",
    "    pp = sns.clustermap(pearson_corr, linewidths=.5, figsize=(50.0, 40.0), cmap='YlGnBu')\n",
    "    plt.setp(pp.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "    plt.savefig(f'img/feature_cluster.svg', bbox_inches = 'tight')\n",
    "    \n",
    "from onekey_algo.custom.components.comp1 import select_feature\n",
    "sel_feature = select_feature(pearson_corr, threshold=0.9, topn=32, verbose=False)\n",
    "\n",
    "sel_feature = sel_feature + labels + ['group']\n",
    "sel_feature\n",
    "\n",
    "sel_data = data[sel_feature]\n",
    "sel_data.describe()\n",
    "\n",
    "alpha = okcomp.comp1.lasso_cv_coefs(X_data, y_data, column_names=None, alpha_logmin=-3)\n",
    "plt.savefig(f'img/feature_lasso.svg', bbox_inches = 'tight')\n",
    "\n",
    "okcomp.comp1.lasso_cv_efficiency(X_data, y_data, points=50, alpha_logmin=-3)\n",
    "plt.savefig(f'img/feature_mse_label.svg', bbox_inches = 'tight')\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "models = []\n",
    "for label in labels:\n",
    "    clf = linear_model.Lasso(alpha=alpha)\n",
    "    clf.fit(X_data, y_data[label])\n",
    "    models.append(clf)\n",
    "    \n",
    "COEF_THRESHOLD = 1e-8 # 筛选的特征阈值\n",
    "scores = []\n",
    "selected_features = []\n",
    "for label, model in zip(labels, models):\n",
    "    feat_coef = [(feat_name, coef) for feat_name, coef in zip(column_names, model.coef_) \n",
    "                 if COEF_THRESHOLD is None or abs(coef) > COEF_THRESHOLD]\n",
    "    selected_features.append([feat for feat, _ in feat_coef])\n",
    "    formula = ' '.join([f\"{coef:+.6f} * {feat_name}\" for feat_name, coef in feat_coef])\n",
    "    score = f\"{label} = {model.intercept_} {'+' if formula[0] != '-' else ''} {formula}\"\n",
    "    scores.append(score)\n",
    "    \n",
    "print(scores[0])\n",
    "\n",
    "\n",
    "feat_coef = sorted(feat_coef, key=lambda x: x[1])\n",
    "feat_coef_df = pd.DataFrame(feat_coef, columns=['feature_name', 'Coefficients'])\n",
    "feat_coef_df.plot(x='feature_name', y='Coefficients', kind='barh')\n",
    "\n",
    "plt.savefig(f'img/feature_weights.svg', bbox_inches = 'tight')\n",
    "\n",
    "feat_coef_df.to_csv('TT5_features.csv',index=False)\n",
    "X_data.to_csv('SHAP_features.csv',index=False)\n",
    "\n",
    "model_names = ['SVM', 'KNN', 'RandomForest',  'XGBoost', 'LightGBM', 'NaiveBayes', 'AdaBoost', 'GradientBoosting', 'LR', 'MLP']\n",
    "models = okcomp.comp1.create_clf_model(model_names)\n",
    "model_names = list(models.keys())\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "from onekey_algo.custom.components.comp1 import plot_feature_importance, smote_resample\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from onekey_algo.custom.components.comp1 import plot_feature_importance, plot_learning_curve, smote_resample\n",
    "\n",
    "targets = []\n",
    "os.makedirs('models', exist_ok=True)\n",
    "for l in labels:\n",
    "    new_models = okcomp.comp1.create_clf_model_none_overfit(model_names)\n",
    "    new_models['LR'] = LogisticRegression(penalty='none', max_iter=100)\n",
    "    new_models['SVM'] = SVC(penalty=100,gamma='auto',kernel='linear')\n",
    "    new_models['RandomForest'] = RandomForestClassifier(n_estimators=50, max_depth=4) \n",
    "    new_models['KNN'] = KNeighborsClassifier(n_neighbors=5, weights='uniform',metric='euclidean')\n",
    "    new_models['XGBoost'] = XGBClassifier(n_estimators=6, objective='binary:logistic', max_depth=3, min_child_weight=0.2,use_label_encoder=False, eval_metric='error')\n",
    "    new_models['AdaBoost'] = AdaBoostClassifier(n_estimators=50, learning_rate=0.1,base_estimator__max_depth=3)\n",
    "    new_models['LightGBM'] = LGBMClassifier(n_estimators=20, max_depth=4, min_child_weight=0.5,)\n",
    "    new_models['NaiveBayes'] =GaussianNB(alpha=1)\n",
    "    new_models['GradientBoosting'] = GradientBoostingClassifier(n_estimators=200,max_depth=4, learning_rate=0.1, min_samples_split=4,min_samples_leaf=1, max_features='sqrt')\n",
    "    new_models['MLP'] = MLPClassifierr(hidden_layer_sizes=(50, 50),activation='relu',solver='sgd',alpha= 0.01,learning_rate_init=0.01)\n",
    "#模型筛选\n",
    "    \n",
    "    model_names = list(new_models.keys())\n",
    "    new_models = list(new_models.values())\n",
    "    for mn, m in zip(model_names, new_models):\n",
    "        X_train_smote, y_train_smote = X_train_sel, y_train_sel\n",
    "        # 取消下一行的注释可以使用Smote进行采样，解决样本不均衡的问题。\n",
    "#         X_train_smote, y_train_smote = smote_resample(X_train_sel, y_train_sel)\n",
    "        m.fit(X_train_smote, y_train_smote[l])\n",
    "        # 保存训练的模型\n",
    "        joblib.dump(m, f'models/{mn}_{l}.pkl') \n",
    "        # 输出模型特征重要性，只针对高级树模型有用\n",
    "        plot_feature_importance(m, selected_features[0], save_dir='img')\n",
    "        \n",
    "#         plot_learning_curve(m, X_train_sel, y_train_sel, title=f'Learning Curve {mn}')\n",
    "#         plt.savefig(f\"img/Rad_{mn}_learning_curve.svg\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "    targets.append(new_models)\n",
    "    \n",
    "    \n",
    "    \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from onekey_algo.custom.components.delong import calc_95_CI\n",
    "from onekey_algo.custom.components.metrics import analysis_pred_binary\n",
    "\n",
    "predictions = [[(model.predict(X_train_sel), model.predict(X_test_sel)) \n",
    "                for model in target] for label, target in zip(labels, targets)]\n",
    "pred_scores = [[(model.predict_proba(X_train_sel), model.predict_proba(X_test_sel)) \n",
    "                for model in target] for label, target in zip(labels, targets)]\n",
    "\n",
    "metric = []\n",
    "pred_sel_idx = []\n",
    "for label, prediction, scores in zip(labels, predictions, pred_scores):\n",
    "    pred_sel_idx_label = []\n",
    "    for mname, (train_pred, test_pred), (train_score, test_score) in zip(model_names, prediction, scores):\n",
    "        # 计算训练集指数\n",
    "        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_train_sel[label], \n",
    "                                                                                              train_score[:, 1])\n",
    "        ci = f\"{ci[0]:.4f} - {ci[1]:.4f}\"\n",
    "        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f\"{label}-train\"))\n",
    "                 \n",
    "        # 计算验证集指标\n",
    "        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_test_sel[label], \n",
    "                                                                                              test_score[:, 1])\n",
    "        ci = f\"{ci[0]:.4f} - {ci[1]:.4f}\"\n",
    "        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f\"{label}-test\"))\n",
    "        # 计算thres对应的sel idx\n",
    "        pred_sel_idx_label.append(np.logical_or(test_score[:, 0] >= thres, test_score[:, 1] >= thres))\n",
    "    \n",
    "    pred_sel_idx.append(pred_sel_idx_label)\n",
    "metric = pd.DataFrame(metric, index=None, columns=['model_name', 'Accuracy', 'AUC', '95% CI',\n",
    "                                                   'Sensitivity', 'Specificity', \n",
    "                                                   'PPV', 'NPV', 'Precision', 'Recall', 'F1',\n",
    "                                                   'Threshold', 'Task'])\n",
    "\n",
    "sel_model = model_names\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "    \n",
    "        # Plot all ROC curves\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for pred_score, label in zip(pred_scores, labels):\n",
    "            okcomp.comp1.draw_roc([np.array(y_train_sel[label]), np.array(y_test_sel[label])], \n",
    "                                  pred_score[sel_model_idx], \n",
    "                                  labels=['Training', 'Internal validation'], title=f\"Model: {sm}\")\n",
    "            plt.savefig(f'img/model_{sm}_roc.svg', bbox_inches = 'tight')\n",
    "            \n",
    "sel_model = model_names\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_test_scores = []\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            pred_test_scores.append(pred_score[sel_model_idx][1])\n",
    "    okcomp.comp1.draw_roc([np.array(y_test_sel[label])] * len(pred_test_scores), \n",
    "                          pred_test_scores, \n",
    "                          labels=sel_model, title=f\"Internal validation cohort AUC\")\n",
    "    plt.savefig(f'img/model_roc.svg', bbox_inches = 'tight')\n",
    "    \n",
    "sel_model = model_names\n",
    "\n",
    "\n",
    "from onekey_algo.custom.components.comp1 import plot_DCA\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_test_scores = []\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            okcomp.comp1.plot_DCA(pred_score[sel_model_idx][1][:,1], np.array(y_test_sel[label]),\n",
    "                                  title=f'{sm} DCA')\n",
    "            plt.savefig(f'img/model_{sm}_dca.svg', bbox_inches = 'tight')\n",
    "    \n",
    "# 设置绘制参数\n",
    "sel_model = model_names\n",
    "c_matrix = {}\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "        for idx, label in enumerate(labels):\n",
    "            cm = okcomp.comp1.calc_confusion_matrix(predictions[idx][sel_model_idx][-1], y_test_sel[label],\n",
    "#                                                     sel_idx = pred_sel_idx[idx][sel_model_idx],\n",
    "                                                    class_mapping={1:'MVI+', 0:'MVI-'}, num_classes=2)\n",
    "            c_matrix[label] = cm\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            plt.title(f'Rad Model:{sm}')\n",
    "            okcomp.comp1.draw_matrix(cm, norm=False, annot=True, cmap='Blues', fmt='.3g')\n",
    "            plt.savefig(f'img/model_{sm}_cm.svg', bbox_inches = 'tight')\n",
    "            \n",
    "sel_model = model_names\n",
    "c_matrix = {}\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "        for idx, label in enumerate(labels):            \n",
    "            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][-1], y_test_sel[label])\n",
    "            plt.title(f'{sm} sample predict score')\n",
    "            plt.legend(labels=[\"label=MVI-\",\"label=MVI+\"],loc=\"lower right\") \n",
    "            plt.savefig(f'img/model_{sm}_sample_dis.svg', bbox_inches = 'tight')\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "sel_model = sel_model\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            target = targets[idx][sel_model_idx]\n",
    "            # 预测训练集和测试集数据。\n",
    "            train_indexes = np.reshape(np.array(train_ids), (-1, 1)).astype(str)\n",
    "            test_indexes = np.reshape(np.array(test_ids), (-1, 1)).astype(str)\n",
    "            y_train_pred_scores = target.predict_proba(X_train_sel)\n",
    "            y_test_pred_scores = target.predict_proba(X_test_sel)\n",
    "            columns = ['ID'] + [f\"{label}-{i}\"for i in range(y_test_pred_scores.shape[1])]\n",
    "            # 保存预测的训练集和测试集结果\n",
    "            result_train = pd.DataFrame(np.concatenate([train_indexes, y_train_pred_scores], axis=1), columns=columns)\n",
    "            result_train.to_csv(f'results/{sm}_Rad_training.csv', index=False)\n",
    "            result_test = pd.DataFrame(np.concatenate([test_indexes, y_test_pred_scores], axis=1), columns=columns)\n",
    "            result_test.to_csv(f'results/{sm}_Rad_test.csv', index=False)\n",
    "            \n",
    "            \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('C:\\\\Users\\\\Admin\\\\Desktop\\\\train\\\\HCC_all_corhort.csv')\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = data.iloc[:, 1:]  # 从第二列开始是特征\n",
    "y = data.iloc[:, 0]   # 第一列是目标变量\n",
    "\n",
    "# SMOTE重采样\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# 初始化10折交叉验证\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# 存储每一折的结果\n",
    "auc_scores = []\n",
    "\n",
    "# 交叉验证\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_resampled, y_resampled), start=1):\n",
    "    X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "    y_train, y_test = y_resampled[train_index], y_resampled.iloc[test_index]\n",
    "    \n",
    "    # 初始化GradientBoosting模型\n",
    "    model = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 预测概率\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # 计算AUC\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_scores.append({'Fold': fold, 'AUC': auc})\n",
    "\n",
    "# 将结果转换为DataFrame\n",
    "results_df = pd.DataFrame(auc_scores)\n",
    "\n",
    "# 保存到CSV文件\n",
    "output_path_csv = 'C:\\\\Users\\\\Admin\\\\Desktop\\\\AUC_Scores.csv'\n",
    "results_df.to_csv(output_path_csv, index=False)\n",
    "\n",
    "# 提取AUC分数列表\n",
    "auc_scores_list = [item['AUC'] for item in auc_scores]\n",
    "\n",
    "# 计算平均AUC和标准差\n",
    "mean_auc = np.mean(auc_scores_list)\n",
    "std_auc = np.std(auc_scores_list)\n",
    "\n",
    "# 绘制AUC曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(auc_scores_list, marker='o', linestyle='-', color='b')\n",
    "plt.fill_between(range(1, len(auc_scores_list)+1), [mean_auc - std_auc]*len(auc_scores_list), [mean_auc + std_auc]*len(auc_scores_list), color='b', alpha=0.2)\n",
    "plt.title('10-Fold Cross-Validation AUC Scores of Training Cohort')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.grid(True)\n",
    "plt.savefig('C:\\\\Users\\\\Admin\\\\Desktop\\\\AUC_Curve.svg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import numpy as np\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('C:/Users/Admin/Desktop/K折交叉验证结果/DeLong_Test/Training.csv')\n",
    "\n",
    "# 目标分类变量和两个预测变量\n",
    "y = data['label']\n",
    "y_pred1 = data.iloc[:, 1]  # 第二列变量\n",
    "y_pred2 = data.iloc[:, 2]  # 第三列变量\n",
    "\n",
    "# 计算AUC\n",
    "auc1 = roc_auc_score(y, y_pred1)\n",
    "auc2 = roc_auc_score(y, y_pred2)\n",
    "\n",
    "# 计算敏感性、特异性、阳性预测值、阴性预测值及其95%置信区间\n",
    "fpr1, tpr1, _ = roc_curve(y, y_pred1)\n",
    "fpr2, tpr2, _ = roc_curve(y, y_pred2)\n",
    "\n",
    "# 计算最佳阈值（Youden's J statistic）\n",
    "j_statistic1 = tpr1 - fpr1\n",
    "j_index1 = np.argmax(j_statistic1)\n",
    "threshold1 = fpr1[j_index1]\n",
    "\n",
    "j_statistic2 = tpr2 - fpr2\n",
    "j_index2 = np.argmax(j_statistic2)\n",
    "threshold2 = fpr2[j_index2]\n",
    "\n",
    "# 计算敏感性、特异性、阳性预测值、阴性预测值\n",
    "sensitivity1 = tpr1[j_index1]\n",
    "specificity1 = 1 - fpr1[j_index1]\n",
    "ppv1 = sensitivity1 / (sensitivity1 + (1 - specificity1) * (1 - threshold1))\n",
    "npv1 = specificity1 / (specificity1 + threshold1 * (1 - sensitivity1))\n",
    "\n",
    "sensitivity2 = tpr2[j_index2]\n",
    "specificity2 = 1 - fpr2[j_index2]\n",
    "ppv2 = sensitivity2 / (sensitivity2 + (1 - specificity2) * (1 - threshold2))\n",
    "npv2 = specificity2 / (specificity2 + threshold2 * (1 - sensitivity2))\n",
    "\n",
    "# 计算95%置信区间\n",
    "conf_int1 = proportion_confint(ppv1, ppv1 + npv1, method='wilson')\n",
    "conf_int2 = proportion_confint(ppv2, ppv2 + npv2, method='wilson')\n",
    "\n",
    "# 打印结果\n",
    "print(f'AUC1: {auc1:.3f}, 95% CI: {conf_int1}')\n",
    "print(f'AUC2: {auc2:.3f}, 95% CI: {conf_int2}')\n",
    "print(f'Sensitivity1: {sensitivity1:.3f}, Specificity1: {specificity1:.3f}, PPV1: {ppv1:.3f}, NPV1: {npv1:.3f}')\n",
    "print(f'Sensitivity2: {sensitivity2:.3f}, Specificity2: {specificity2:.3f}, PPV2: {ppv2:.3f}, NPV2: {npv2:.3f}')\n",
    "\n",
    "# DeLong检验比较两个AUC\n",
    "from scipy.stats import norm\n",
    "\n",
    "def delong_test(auc1, auc2, n1, n2):\n",
    "    se1 = np.sqrt((auc1 * (1 - auc1) / n1) + (auc2 * (1 - auc2) / n2))\n",
    "    se2 = np.sqrt((auc2 * (1 - auc2) / n2) + (auc1 * (1 - auc1) / n1))\n",
    "    z = (auc1 - auc2) / np.sqrt(se1**2 + se2**2)\n",
    "    p = 2 * (1 - norm.cdf(abs(z)))\n",
    "    return z, p\n",
    "\n",
    "z, p = delong_test(auc1, auc2, len(y), len(y))\n",
    "print(f'DeLong Test: Z = {z:.3f}, P = {p:.3f}')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr1, tpr1, label=f'Combined model AUC = {auc1:.3f}')\n",
    "plt.plot(fpr2, tpr2, label=f'Clinical model AUC = {auc2:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # 绘制对角线\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Test cohorts curves')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# 保存ROC曲线到桌面\n",
    "desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "roc_plot_path = os.path.join(desktop_path, 'ROC_Curves.svg')\n",
    "plt.savefig(roc_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f'ROC curves saved to: {roc_plot_path}')\n",
    "\n",
    "# SHAP可视化\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "model_path = r'D:\\onekey\\onekey_comp\\comp2-结构化数据\\models/GradientBoosting_label.pkl'\n",
    "feature_path = r'D:/onekey\\onekey_comp/comp2-结构化数据/SHAP_features.csv'\n",
    "analysis_label = 1\n",
    "analysis_sample = 150\n",
    "shap_value = get_shap_value(shap_values, analysis_label=analysis_label, samples=analysis_sample)\n",
    "shap.plots.waterfall(shap_value, max_display=10, show=False)\n",
    "plt.savefig(f\"img/{analysis_sample}_waterfall.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "shap_value.feature_names = [f.replace('_', '\\n') + '\\n' for f in shap_value.feature_names]\n",
    "shap.plots.force(shap_value, show=False, matplotlib=True, figsize=(20, 4))\n",
    "plt.savefig(f\"img/{analysis_sample}_force.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "shap_value = get_shap_value(shap_values, analysis_label=analysis_label, samples=analysis_sample)\n",
    "shap.plots.waterfall(shap_value, max_display=10, show=False)\n",
    "plt.savefig(f\"img/{analysis_sample}_waterfall.svg\", bbox_inches='tight')\n",
    "plt.show()\n",
    "shap_value.feature_names = [f.replace('_', '\\n') + '\\n' for f in shap_value.feature_names]\n",
    "shap.plots.force(shap_value, show=False, matplotlib=True, figsize=(20, 4))\n",
    "plt.savefig(f\"img/{analysis_sample}_force.svg\", bbox_inches='tight')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
